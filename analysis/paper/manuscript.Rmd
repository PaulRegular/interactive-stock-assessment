---
title: Improving the communication and transparency of stock assessment using interactive
  visualization tools
author: "Paul M Regular, Gregory J Robertson, Keith P. Lewis, etc?"
date: "Fisheries and Oceans Canada, Northwest Atlantic Fisheries Center, 80 East White Hills, St. John's, Newfoundland and Labrador, A1C 5X1, Canada"
output: 
  bookdown::word_document2:
    reference_docx: template.docx
bibliography: references.bib
---

Quantitative stock assessment plays a central role in modern fisheries management. Over time and as new methods are developed, biologists have more and more data available to inform stock assessments. These data might be richer information on stock structure using an increasing array of markers and biomarkers, and/or the continued lengthening of population status and catch time series. Concurrently, analysis methods have improved, and techniques are now available that can integrate many data sources into one stock assessment model; state-space models being one popular class of such models. For well-monitored stocks, the challenge has shifted from having sufficient data and information to provide sound advice on the status of a stock, to presenting the mass of information produced by increasingly complex statistical models to end-users in a meaningful way. Traditional methods of presenting information at stakeholder meetings and to fisheries managers, like written reports full of tables, or with presentations with endless figures, are simply no longer sufficient to convey the richness of the information. More importantly, these static presentation formats tend to stifle meaningful discussion of model outputs, and important questions and assumptions of the models that should be discussed tend to get lost under the sheer volume of output. A solution to this problem of communicating complex output and results to encourage engaging discussion can be through using the effective use of interactive visualization tools. These are common tools we use every day on an endless array of web sites, but their use is no longer restricted to web site developers as these tools are being integrated into software commonly used by the research community.

Here we aim to demonstrate how interactive visualization tools provide an efficient and effective means of exploring and communicating the ever expanding array of data inputs and model outputs. First, we focus on data that are commonly used in stock assessment and provide one example where interactive maps were used to simplify the detailed exploration of data from a long-term tagging program. Second, we focus on the modeling aspect of stock assessment and, again, we use an example to demonstrate how dynamic and interactive data visualizations can be used to explore, diagnose and communicate results from an integrated assessment model. This structure corresponds to the two general steps in the stock assessment process: 1) knowing the data, and 2) data modeling.

# Knowing the data

The data sets used in stock assessments are constantly growing. This growth in data either stems from the continuation of long-term monitoring efforts or from the addition of new monitoring programs. As such, stock assessment biologists often have to manage large volumes of data from a variety of sources. For instance, time series of reported landings and catch-at-age are "fishery-dependent" data that are frequently used in stock assessments. These data are often analyzed in conjunction with data from "fishery-independent" surveys that track changes in abundance and, in many cases, also monitor trends in biological factors such as age composition, growth rates, sex ratios and maturation stages. For some data-rich stocks, mark and recapture studies are also carried out to estimate movement, migration, growth rate, natural mortality, and discard mortality. All of the above-mentioned data sets are multidimensional and as the volume and variety of these data increases, it becomes more difficult to be aware of the details of each data set and discover key patterns within each. This challenge can be mediated, to a degree, by the application of interactive visualization tools as that they allow detailed exploration of the data behind the plot. For instance, the ability to zoom in on features or areas of interest and hover over specific points to reveal more information creates an interactive user-driven experience that expedites explorations of the data. This is exemplified by an interactive mapping tool developed for the exploration of a long-term tagging study.

> Greg, can you introduce this tool, providing some info on the tagging database and the tools used to make the tool (Shiny, leaflet)?

> TODO: the intro paragraph to this section could use some references if it is necessary

# Data modeling

Synthesizing data from multiple sources presents a key challenge to stock assessment. Analyses of different data sources were traditionally carried out independently and the summaries or parameters from these analyses were used in the assessment model. This approach, however, is less than ideal because information may be lost and uncertainty may be unaccounted for when we "do statistics on the statistics" [@link1999; @maunder2013]. Such issues have largely been curtailed in contemporary stock assessments thanks to advances in software that have facilitated the analysis of all available data, in as raw a form as appropriate, in a single integrated analysis [@maunder2013]. Specifically, statistical modeling tools such as JAGS [@plummer2003], AD Model Builder [@fournier2012] and Template Model Builder [@kristensen2015] allow the construction of a joint likelihood for an array of observations to, in theory, extract as much information as possible about the biological and fishery processes. From a computational perspective, analyses of a variety of large data sets has never been easier. However, from a human perspective, contemporary stock assessment biologists are faced with the challenge of managing data from a variety of sources and also understanding the algorithms that convert these data to advice for fisheries managers. By using a recently developed interactive tool for exploring the integrated assessment model for Northern cod as an example, we hope to demonstrate that such tools can streamline the process of understanding and communicating results from complex stock assessment models.

The Northern cod stock off southern Labrador and eastern Newfoundland is one of the most well studied stocks in eastern Canada, perhaps by virtue of its history. As such, there are multiple monitoring programs that help inform the status of the stock and data from most of these programs have been integrated into a state-space stock assessment model, called NCAM [@cadigan2016]. The "base case" model includes information from research vessel autumn trawl surveys (1983-present), Sentinel fishery surveys (1995-present), inshore acoustic surveys (1995-2009), fishery catch-at-age compositions and partial fishery landings (1983-present), and tagging (1983-present). Using a series of observation equations, this TMB based model reduces thousands of historical data points from these monitoring programs into quantities such as recruitment, spawning stock biomass, fishing mortality and natural mortality. Once the model is fit to the data, the next step is to produce visual representations of the data and model. A standard approach would be to produce static presentations and documents with a series of figures and tables. With large amounts of model inputs and outputs, this approach quickly becomes overwhelming for both the analysist and all the stakeholders involved in the stock assessment process. First, it is not feasible for the analysist to include and describe all figures and tables produced into a single document. Second, it is difficult for stakeholders to efficiently digest the information that has been compressed into a series of static slides or pages. Interactive documents provide a potential solution to this problem as they allow much more information to be contained and assessible on a single screen. 

In an effort to make the results from the Northern cod assessment model more accessible and transparant, an interactive and self-contained "dashboard" was developed for the last assessment of Northern cod [@dwyer2019]. The concept of using a dashboard was burrowed from the business community where dashboards are frequently used to group a series of interactive visuals and tables to provide at-a-glance views of key performance indicators. Many tools have been developed to facilitate the development of data dashboards and we leveraged R-based tools to construct a tool for exploring the input and output of NCAM. Specifically we utilized the flexdashboard [@allaire2017] package to group interactive plotly-based [@plotly] visuals into a dynamic document. We also used the crosstalk [@cheng2016] package to link the data displayed across multiple plots.

The NCAM dashboard, which is included in Supplement 2 as a self-contained html file, contains a series of tabs, the first of which provides terse point-form background on the model (tab named "Background"). Subsequent tabs provide a series of diagnostic plots for assessing model fits to the catch ("Catch"), research vessel autumn trawl survey ("RV survey"), Sentinel fishery survey ("SN survey"), inshore acoustic survey ("SS survey"), and tagging ("Tagging") data. For instance, the "RV survey" tab includes plots of observed and predicted values of mean numbers per tow captured in the research vessel survey (Figure \@ref(fig:rv)). The dashboard also includes tabs focused on model estimates such as catchability and selectivity ("Catchability"), stock size and vital rates ("Trends"; Figure \@ref(fig:trends)), and stock productivity ("Productivity"). Finally, some results from a retrospective analyses are included under the "Retro" tab and key inputs and outputs are included as tables under the "Tables" tab. The plots and tables included in the dashboard are similar to those typically presented at assessment meetings and in research documents, however, the interactive nature of the dashboard permits easy and efficient access to significantly more detail. Because the dashboard is contained in an html file, it acts as an interactive assessment document that can be shared for peer-review. This allows colleagues and stakeholders to independently scrutinize details of the data and model that are not easily accessed by users other than the analyst. Such access improves the transparency of the stock assessment model which, in turn, leads to richer discussion and interogation of the biological and statistical rigor of the model.

> TODO: should I add detail of questions not previously raised? If so, I can't quite remember.


```{r rv, echo = FALSE, fig.cap = '- Screenshot of the "RV survey" tab from the NCAM dashboard where total observed (dots) and NCAM model predicted values (lines) for the DFO RV survey index are shown in the upper left panel and scaled matrix plot of age-disaggregated standardized log residuals are shown in the lower left panel (blue = positive, red = negative, symbols scaled by size; grey = index values of zero). Age-disaggregated observed (dots) and predicted (lines) values from the DFO RV survey are shown in the right panel. The tab on the right panel including residual plots are hidden here, but assessable in the dashboard.'}
knitr::include_graphics("figures/NCAM_dashboard_rv-survey_tab.png")
```


```{r trends, echo = FALSE, fig.cap = '- Screenshot of the "Trends" tab which displays estimates of recruitment, stock size, and stock size relative to B~lim~ (left panels) and mortality rates (F, M, Z, right panels).'}
knitr::include_graphics("figures/NCAM_dashboard_trends_tab.png")
```


# Open stock-assessment

decades as has the complexities of data management, exploratory data analysis, as well as the formal analyses and associated diagnostics.  The majority of this sequence of events, sometimes called “the data pipeline”, have not traditionally been part of the peer-review process which sees only the end products.  However, decisions made at these steps increasingly influence the outcome of the study.  Gelman coined the term “the forked garden path” to illustrate that different conclusions can be arrived at depending on what decisions one makes along the garden path.  Due to a number of limitations, such as available pages in journals, much of the data pipeline is not transparent nor is it reproducible, even if it is one’s own analysis.  A number of authors have recently advocated for a culture of open science and reproducible research, i.e., a change in the transparency and reproducibility of science.  Proponents of open science and reproducible research posit a number of benefits including a more productive and responsible scientific culture, an ability to address larger and more complex questions, as well as a more efficient workflow and ability to reproduce one’s own work (Lewis et al. 2018).  

We submit that the dashboard tool is also a major step forward in terms of open science and reproducible research in addition to being a major step forward in how stock assessments are presented and critiqued.  Consider the following process.  Through the Canadian Science Advisory Secretariat (CSAS), DFO Science provides peer-reviewed science advice to fisheries managers at regular meetings (Regional Assessment Process).  These meetings are attended by a variety of stake holders including harvesters, academics, and NGOs.  Documents summarizing these processes (research documents, stock assessment reports, and proceedings summaries) are available to the public through the CSAS website.  Ergo the CSAS process strives to uphold the ideal of transparency.  However, as it currently stands, only the end products of an assessment are open and transparent; as with virtually all scientific analyses, the data pipeline of stock assessment is hidden from review.  

With the exception of data management, the dashboard removes much this problem – all analyses and diagnostics can be readily reviewed.  In terms of data management, this point is largely ameliorated because the dashboard tool is constructed in Program R using R Markdown and the code for creating the dashboard and all associated analyses are available on Github.  Data are also available through information requests.  So the entire data pipeline is transparent as well as reproducible, i.e., the requirements of open science are fulfilled.

The dashboard also promotes accessibility to the stock assessment.  Even if one is able to download code from a content management system (e.g., GitHub or Bitbucket), submit a data request, and manipulate the data so that it conforms to the code, it is unlikely that an individual has the time to fully recreate a particular analysis in its entirety when most reviewers are already overcommitted (Banks 20XX). In the case of NCAM, considerable statistical experience and expertise is required simply to run the model (or perhaps to even tweak the model).  However, the dashboard removes many of these difficulties.  The html file contains all of the relevant analyses.  The number of analyses can be expanded upon request and can be distributed prior to meetings and reviewers can assess the results and diagnostics at their leisure.

We submit that the plotly/flex dashboard tool is an important step forward in transparent and reproducible science.  Further, this tool makes stock assessments more accessible to stakeholders.  Beyond stock assessments, it this tool could be used for a more fulsome review of manuscripts (i.e., reviewers can now assess claims like “we examined the residuals and found that the assumptions of the model were met”) in many fields.  



> **Other draft text**

> A wide array data may be collected for an assessment and, formally, the process often reduces to algorithms that convert these data to advice for fisheries managers. In some cases, particularly for commercially valuable species, this means that hundreds or thousands of historical data points from the monitoring program of a stock gets reduced into a single policy value, such as a recommended catch quota [@maunder2009]. Sound management advice therefore hinges on the ability of stock assessment biologists to manage, interpret and communicate large volumes of data from a variety of sources. 

> In this data-intensive era of stock assessment, one of the difficulties becomes visualizing the inputs and outputs to integrated stock assessment models. The standard approach has been to create a series of tables and static plots to help assess the inputs and model fits. Data presented in tables is incredibly valuable, but as human beings, seeing patterns can be challenging. Static figures, in contrast, help reveal patterns but they are divorced from the underlying data and, as such, limit detailed explorations. New tools are bridging this gap by making it easier to build interactive figures [@perkel2018].

> **Links to some handy references**

> Interactive visuals...visual representation of the data and model that help us interpret complex data at a glance https://campus.sagepub.com/blog/3-benefits-of-interactive-visualization

> https://centricdigital.com/blog/digital-trends/the-value-of-interactive-data-visualization-and-why-it-matters-to-business/

> https://www.nature.com/articles/s41467-018-03297-7

> https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/120103

> https://www.nature.com/articles/d41586-018-01322-9

> "The ultimate objective of Fisheries Science is to inform management" (http://www.fao.org/tempref/docrep/fao/008/a0212e/a0212E02.pdf)


# References
